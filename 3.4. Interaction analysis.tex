\subsection{Interaction analysis}

The software Trentool (\cite{3.4.Trentool}, version 3.3.1beta, 30 May 2015) was used with Matlab 2013a for exploring entropy transfers between cortical areas.
For this purpuse, data was prepared for each subject individually.
The procedure consisted of three sections.

\paragraph{Input preparation}
During the first procedural section, the input datasets were prepared for the analysis.
This preparation consisted of three components: regional activity from single trials, a list of regional comparisons and a set of parameters.

For the first component, two sets of single trials were selected from each subject.
Mean regional activity were then extracted from each trial.
Cortical regions were selected if they showed syntax effects either in the cluster analysis or the interval analysis.
This process yielded six relevant cortical regions: PAC, aSTG, pSTS, BA44, BA45 and BA6v.
Data from these regions were lowpass filtered with a digital, single-pass FIR filter (passband frequency of 250Hz, stopband frequency of 300Hz, ripple of 1db, stopband attenuation of 120db; implemented with the \emph{filter} function in Matlab) to suppress the influence from the HPI coils.
This filter introduced a signal lag of 79ms, and selected time windows were shifted accordingly.

For the second component, I selected only the most relevant functional connections between cortical regions.
There are 720 unique possiblities to compare these regions, which would overwhelm\footnote{The comparison over one pair of cortical regions, in forward and reverse directions, over two conditions and 50 separate shift delays, required a computation time of 35 CPU cores * hours per subject and timewindow. Calculating transfer entropy between 36 pairs of regions took a total of 3150 CPU cores * hours, which - thanks to the work of Christian Labadie and Hermann Sonntag - amounted to 48 actual hours of calculation time. The task of computing the full set of 720 comparisons would have required 63000 CPU cores * hours - for other groups, a sufficient reason to switch a GPU-based algorithm\cite{3.4.gpuTE}.} the computational capacity at my disposal. For this reason, I limited the calculation to comparisons between regions along the same axonal fiber bundle.

I selected comparisons between all language-related regions along three pathways (see Fig. \ref{3.4.regions}) that are known to be involved in subject-object-paradigms.
The following regions associated to each pathway are derived from \cite{1.1.pathways}.

The second dorsal language pathway, via the arcuate fascicle, connects the regions PAC, pSTG and BA44.
This pathway is therefore represented by two short connections ($PAC_{lh} \rightarrow pSTG_{lh}$ and $pSTG_{lh} \rightarrow BA44_{lh}$) and one long connection ($PAC_{lh} \rightarrow BA44_{lh}$).

The second ventral language pathway, via the inferior fronto-occipital fascicle, connects the regions PAC, aSTG and BA45.
This layout yields two short connections ($PAC_{lh} \rightarrow aSTG_{lh}$, $aSTG_{lh} \rightarrow BA45_{lh}$) and one long connection, $PAC_{lh} \rightarrow BA45_{lh}$.

The first dorsal language pathway, via the nomen nescio and the third component of the superior longitudinal fascicle, connects the regions pSTG, SMG and BA6v.
SMG is a substantially larger region than pSTG and BA6v, and is apparently responsible for sensory integration.
Task-related effects tend to disappear when averaging over large regions, even when these regions are primarily involved in task-related processing.
Due to this effect, and the minimal role of SMG in object-subject paradigms, I decided to skip the connections $pSTG_{lh} \rightarrow SMG_{lh}$ and $SMG_{lh} \rightarrow BA6v_{lh}$ in the signal transfer calculations.
Consequently, the motor component of the language network will be represented by the long connection $pSTG_{lh} \rightarrow BA6v_{lh}$.

The analysis included two functional regions (BA44/BA45 and aSTG/pSTG) that share a common gyrus.
Since gyri are highly interconnected, a certain degree of information exchange can be expected.
Connections between those regions were examined as well, yielding another two short connections ($BA44_{lh} \rightarrow BA45_{lh}$ and $aSTG_{lh} \rightarrow pSTG_{lh}$).

In order to compute forward and backward directions, two separate calculations are necessary for each comparison.
Therefore, examing the information transfer along these nine regional connections requires 18 sets of calculations.


\begin{figure}[h]
\begin{center}
\vspace{7mm}
\includegraphics[width=0.75\textwidth]{pics/3_4_regions.png}
\caption{\label{3.4.regions} Connections of selected cortical regions for the computation of transfered entropy. Left of PAC: ventral pathway; right of PAC: dorsal pathways 1 and 2.}
\end{center}
\end{figure}

The third component was the set of parameters for the data preparation.

First, I needed to define a time interval that is analyzed for transfer entropy.
The interval analysis showed (see chapter \ref{4.3}) that syntax effects occurred predominantly up to 1000ms after onset.
These effects were based on group analysis, while the following interaction analysis was based on a single-subject, single-trial analysis.
Due to these circumstances, I decided against limiting the interaction analysis to the previously discovered time intervals, and for exploring the full time range between 0ms and 1000ms (after stimulus onset) instead.
As a reasonable tradeoff between time resolution and data content, I fragmented the analysis into five segments of 200ms.

Next, I needed to choose embedding parameters.
To understand embedding, it is important to know that the transfer entropy estimation fundamentally relies on the comparison between two probability density functions.
These mathematical constructs can only be approximated from scalar time series.
For this purpose, time series from individual trials are combined into a multidimensional state space by TRENTOOL.
The reduction of this multidimensional space into a probability density function is the process called embedding.
There are four important embedding parameters: the embedding criterion, the embedding time, the embedding delay range and the embedding dimension range.

For choosing suitable parameter ranges, I followed the recommendations described in the Trentool tutorial \cite{3.3.TrentoolTutorial}.
As for the embedding criterion, I selected the Ragwitz criterion (\emph{cfg.optimizemethod = 'ragwitz'}).
It is recommended for EEG/MEG data, while the only other option - the Cao Criterion - is recommended for fMRI data.

The embedding delay is denominated in multiples of the autocorrelation time (ACT).
The embedding setup relies heavily on this metric.
The ACT is defined as the first positive minimum of the self-correlation of the signal.
For my set of single trials, the median ACT was 2ms (90th: 6ms, 95th: 10ms, 99th: 22ms, 99.9th: 34ms).
The usable embedding interval depends on the maximum ACT, but the embedding interval must be identical for all datasets.
Hence, a single trial with a high ACT can reduce the effective embedding interval for all other trials.
I set the parameters \emph{cfg.trialselect = 'ACT'} and \emph{cfg.actthrvalue = 12} so that trials with an ACT greater than 12 were excluded from the analysis.
This setting caused a rejection of 3.4\% of all trials.
The relatively high rejection rate was required to ensure that the maximum embedding interval wouldn't exceed 1000 samples in combination with the following embedding delay and embedding dimension.

As recommended, I set the embedding delay to a search range between 0.2 and 0.5 times of the autocorrelation delay, (\emph{cfg.ragtaurange = 0.2:0.5}).
A preliminary analysis with a embedding dimension range of 2 to 12 yielded 12 as the most common embedding dimension, indicating a ceiling effect.
To alleviate this ceiling effect, I extended the maximum embedding dimension by two (\emph{cfg.ragdim = 7:14}).

For the embedding process, a considerable section of data is necessary to estimate the baseline entropy before the interaction time cue occurs.
This section of data is referred to as embedding time, and won't be included in the analysis of transfer entropy.
It is necessary to define the embedding time and TOI with a combination of two variables, \emph{cfg.toi} and \emph{cfg.repPred}.
repPred denominates the length of the TOI that is actually used for the information transfer calculation.
With a sampling rate of 1000 samples per second and a desired time window of 200ms, I set it to \emph{cfg.repPred = 200}.
Due to an implementation quirk in Trentool, selecting a fixed data interval for the signal transfer calculation is not possible\footnote{According to the comment section of \emph{transferentropy.m}: "The span of time needed for embedding is: (max(dim)-1)*max(tau). The prediction time starts after this embedding time. Hence the span of time defined in cfg.toi must be a good deal longer than the embedding time, at least embedding time plus 150 samples or max(cfg.predicttime_u)."
Since tau depends on ACT, which in turn is different for each data set, it is impossible to compute a fixed embedding time for all data sets.}. The Trentool authors expect their users to set a very broad TOI, leaving enough samples to both include the relevant data and the embedding time in all cases.
This implementation leads to a sliding TOI (fixed length, but with varying onset) for the actual information transfer calculations.
Since the intention was to explore signal interactions at clearly defined time intervals, the Trentool implementation was too imprecise for my goals.
In order to achieve a higher degree of control across trials and subjects, I modified the routine \emph{transferentropy.m} (for a detailed explanation, see \ref{Appendix.A}) to use a fixed TOI for its calculations.
After the modification, Trentool expanded the start of the TOI dynamically with the length of the embedding interval.
To achieve this effect, \emph{cfg.toi} was set to the start and end of the TOI, plus a fixed filter delay of 79ms.
\emph{cfg.repPred} was set to the length of the TOI in samples, i.e., 200.

With these parameter ranges, Trentool used the Ragwitz criterion to optimize the embedding parameters for each set of time series (once per subject, time window and condition).
The most common optimal embedding dimension was 14, and the optimal relative embedding delay was 0.44.
The minimal embedding interval for this combination of parameters was Xms.

After embedding, Trentool performed an interaction shift test.
This procedure (performed with the function \emph{InteractionDelayReconstruction\_calculate()}) counteracts bias introduced by noise, especially the common false positive detection of an interaction between two data sets with different signal-to-noise levels.
The shift test establishes the optimal delay of transfered entropy for each comparison of activity.
Transfer entropy was computed for a time range of possible interaction delays: 1ms to 50ms in steps of 1ms.
TRENTOOL then selected the interaction delay with the biggest associated transfer entropy as the most likely representation of the signal delay caused by cognitive processes and anatomic constraints.
To alleviate issues with volume conduction, I supplied the option \emph{cfg.extracond = 'Faes\_method'}.
Unfortunately, using this method prevents the determination of the precise signal delay.
Since the signal delay is not relevant to my research goals, this trade-off was trivial to solve.
The optimal dimension for each data set was considered with the option \emph{cfg.optdimusage = 'indivdim'}.
For significance testing, TRENTOOL creates surrogate data.
This data was created by shuffling existing trials (\emph{cfg.surrogatetype = 'trialshuffling'}).
The significance level for this procedure was \emph{cfg.alpha = 0.05}.
I used t-values to represent the statistical results of the shift test (\emph{cfg.permstatstype = 'indepsamplesT'}).
These results indicate the likelihood if an entropy transfer has occurred between the selected pair of regional activity.

I then combined these subject-level results in Matlab to obtain group-level results.
First, surrogate test results were combined with Stouffer's combined probability test.
These combined test results were corrected with Benjamini & Hochberg's procedure for controlling the false discovery rate.
Second, signal delay estimations were obtain by calculating the mean over all subjects.


For the third section, I evaluated the impact of the syntax condition on all subjects.
A clustered T-test compared transfer entropy between conditions over all subjects.
For this group-level comparison, the TRENTOOL function \emph{TEgroup\_stats} was used.
One comparison was conducted for each of the three timewindows.