
\chapter{Introduction}

\section{The role of white matter tracts in syntax processing}

\subsection{White fiber tracts}
Brain tries to save space and energy
Connections cost both space and energy, so brain is largely locally organized
Long-distance connections cost much more space and energy than local connections, while not transferring more information
The fact that long-distance connections exist means the transferred information is especially important and/or highly compressed

\subsection{Language network}
Dorsal \& Ventral streams
The ventral stream uses two major long-distance fiber tracts: ECFS and UF
Ventral pathway I (ECFS): STG to BA45
Ventral pathway II (UF): aSTG to FOP
The dorsal stream uses two major long-distance fiber tracts: The arcuate fascicle and the superior longitudinal fascicle.
Dorsal pathway I (SLF): pSTG to premotor cortex in BA6
Dorsal pathway II (AF): pSTG to BA44

During sentence processing, the IFG and posterior superior
 temporal regions have been proposed to exchange information via
 the dorsally located Arcuate Fasciculus/Superior Longitudinal Fasci-
 culus (AF/SLF; Friederici, 2011; Meyer, Obleser, Anwander, \&
 Friederici, 2012; Wilson et al., 2011). This proposal still awaits direct
 causal support, since it is based either on tractography in healthy
 participants (Catani, Jones, \& ffytche, 2005; Friederici, Bahlmann,
 Heim, Schubotz, \& Anwander, 2006; Glasser \& Rilling, 2008; Meyer
 et al., 2012; Parker et al., 2005; Saur et al., 2010; Weiller, Musso,
 Rijntjes, \& Saur, 2009) or data from sentence-processing-impaired
 conduction aphasics whose lesions mostly involve both gray- and
 white-matter damage (Baldo, Klostermann, \& Dronkers, 2008; Buchsbaum et al., 2011; Friedmann \& Gvion, 2003) and primary
 progressive aphasics (Galantucci et al., 2011; Wilson et al., 2010,
 2011). The AF/SLF has been taken to be particularly relevant for the
 processing of sentences with complex word orders (Friederici,
 2011; Wilson et al., 2011).

\section{Functional anatomy of syntax processing}

Region of interest: pSTS
pSTG is involved in complex syntax

Adults:
"'Neuroimaging studies have tried to characterise the neural substrate for representing human action. Many of these studies have followed a different tradition in psychophysics and developmental psychology of investigating the perception of "'biological motion"' - that is, the characteristic articulated motion of chordate animal bodies (e.g. Vaina, Solomon, Chowdhury, Sinha, \& Belliveau, 2001; Grossman \& Blake, 2002; Beauchamp, Lee, Haxby, \& Martin, 2002; Pelphrey, Mitchell, McKeown, Goldstein, Allison, \& McCarthy, 2003) or body and face parts (e.g. Hoffman \& Haxby, 2000; Hooker et al. 2003; Kilts et al. 2003; Pelphrey et al., 2003). Biological motion can also be perceived from the relative motion of just a few dots ("'point-light walkers"', Johansson, 1973); if the dots are spatially or temporally rearranged, the percept is destroyed. These neuroimaging studies suggest that one brain region, the posterior superior temporal sulcus, is particularly involved in the representation of biological motion.
Two sets of recent neuroimaging data suggest that the role of the posterior superior temporal sulcus (pSTS) may extend beyond a response to biological motion, to more abstract representations of intentional action. First, Castelli, Happe, Frith, \& Frith, (2000) and Schultz, Grelotti, Klin, Kleinman, Van der Gaag, Marois, \& Skudlarski, (2003) reported that a region of the pSTS showed a significantly higher response to animations of moving geometric shapes that depicted complex social interactions than to animations depicting inanimate motion. Second, using movies of human actors engaged in structured goal-directed actions (e.g. cleaning the kitchen), Zacks, Braver, Sheridan, Donaldson, Snyder, Ollinger, Buckner, \& Raichle, (2001) found that activity in the pSTS was enhanced when the agent switched from one action to another, suggesting that this region encodes the goal-structure of actions. Both of these results are consistent with a role for a region of pSTS cortex in representing intentional action, and not just biological motion."' (doi:10.1016/j.neuropsychologia.2004.04.015)


"'hierarchy construction might be supported by left inferior frontal (particularly BA44/45) and posterior superior temporal regions: (Ben-Shachar et al., 2003, 2004; Caplan et al., 2002; Just et al., 1996; Röder et al., 2002; Stromswold et al., 1996)"'

"'the enhanced inferior frontal (BA 44/45) activation observed for these sentence types is engendered by highly specialized aspects of syntactic processing, namely by syntactic transformations (Ben-Shachar et al., 2003, 2004; Grodzinsky, 2000)."'

 "'More complex syntactic processing, 
 including the identification of hierarchical structures within a sentence, takes 
 place later at around 300ms to 500ms in the already mentioned pSTG/STS 
 (Cooke et al., 2002; Vandenberghe et al., 2002; Humphries et al., 2005; 
 Bornkessel and Schlesewsky, 2006; Kinno et al., 2008; Friederici et al., 2009; 
 Snijders et al., 2009; Santi and Grodzinsky, 2010; Newman et al., 2010). How-
 ever, this is not achieved by the pSTG/STS alone but, as Friederici argues, 
 critically involves the pars opercularis of the inferior frontal gyrus (IFGoper) in 
 Broca's  area  (Friederici et al., 2006; Makuuchi et al., 2009; Newman et al., 
 2010; Wilson et al., 2010)."' (Skeide 2013)

 ELAN

Children:

"'Children
 performed best on subject relatives, followed by object relatives, indirect
 object relatives, oblique relatives and, finally, genitive relatives. Diessel and
 Tomasello argued that children's superior performance on subject relatives
 reflected a processing effect whereby children prefer to pursue subject-
 extracted interpretations because they have a preference to build simple
 structures, which is based upon their considerable experience with simple
 nonembedded sentences. This is similar to early arguments made by Bever
 (1970, see also Bates \& MacWhinney, 1982; Slobin \& Bever, 1982; Townsend
 \& Bever, 2001), who argued that children use a canonical sentence schema
 (NVN) to interpret sentences"' (Kidd, Brandt, Lieven 2007)

Development of simple sentence processing is complete at age 3 (Friederici 2005)
Kids often don't have the AF tract yet until early adulthood
Previous findings: kids rely more on the ventral pathway
Ventral processing involves BA45 (no condition effect in adults)
Processing is more vulnerable to bias (semantic crosstalk)

\section{Experimental paradigm}

Task: Exploring role and timing of pSTG/pSTS activity in complex syntax processing
Actor/Undergoer setup
Object-/Subject-relative clauses

Sentences are comparable to Constable, 2004:
Subject/object-relative clauses
"'The biologist - who showed the video - studied the snake."'
"'The biologist - who the video showed - studied the snake."'
fMRI effect in:
Left: BA40, 44/45, 39, premotor cortex
Right: BA44/45, premotor cortex

Also comparable to Bornkessel, 2006:
specifically, cases E and F (unambiguous subject- and object-relative clauses with active verb)
significant fMRI effect in:
left: IFG (pars opercularis), pSTG, pSTS, inferior frontal junction, ventral premotor cortex, interparietal sulcus
right: interparietal sulcus

Repeated-measures design
Stationarity assumption

\section{Research questions}

\subsection{Expected discoveries}
Replication of EEG/fMRI results with MEG?
Which cortical regions are involved in the conditional effect?
Is semantic content relevant?
How long does each processing stage take?
In which order do the steps take place?
Can we see a different pathway in kids?

\subsection{Hypotheses}
Condition effect mainly in pSTG, BA44 (adults), BA45 (kids)
Kids: worse performance than adults
Kids: less involvement of pSTG
Adults vs kids: Dorsal II vs. Ventral II\section{Choice of measurement methods}

\subsection{Acquisition}

\paragraph{Neuroanatomic principles}
Neuronal activity creates a combination of electrical and magnetic fields.
On a cellular level, activating a neuron causes depolarization, which in turn creates a weak electric field.
Most neurons are equipped with a long axonal fibre that transmits a relatively strong postsynaptic signal.
Especially in pyramidal cells which are responsible for long-distance transmissions, this axon can span several centimeters in length.
Since signals along axons travel by a complex combination of transmitter binding, ion flux and electric fields, neuron-to-neuron data transmission exhibits three important restrictions.
First, a successful signal transmission requires a short refractory period until the next transmission is possible again.
This leads to the phenomenon that transmissions can only travel one way, and overlapping signals on the same fiber are impossible.
Second, axonal transmissions can only be binary.
If the minimum threshold voltage is reached, the signal will be transmitted at maximum speed along the entire fibre.
Below the threshold, no transmission can occur.
Third, maximum transmission speed is relatively low at approximately $100\frac{m}{s}$.
Other commonly used transmission fibers, for comparison, achieve speeds of $2\cdot10^8\frac{m}{s}$ (copper wire) or even $3\cdot10^8\frac{m}{s}$ (glass fibre).
Additionally, transmision speed depends on the thickness of the fiber, with the thinnest fibers transmitting as slowly as 0.1m/s.
This property causes a considerable delay when transmitting signals over macroscopic distances.
Signal delay in technical applications is generally small enough to be disregarded or considered an inescapable nuisance.
In the brain however, two interconnected neurons at opposite sides of the head can generate output simultaneously, but their signals will be offset by several miliseconds by the time they arrive.
For any neural network, delayed information is therefore an widely expected issue and must be properly factored into signal processing.

\paragraph {Available acquisition methods}
Due to the long axon and the low transmission speed, transmitted signals will create a electrical field that moves along the axon during a non-trivial time window.
Aggregated electric fields from thousands of similar signals can be acquired with an electroencephalograph (EEG).
This process involves measuring the voltage at two or more arbitrary points in the brain; typically, on the surface of the head or the cortex (which is called intracranial EEG, or iEEG).

Every electric signal that travels along a conducting wire also induces a magnetic field.
Since an axon is no exception to this rule, any transmitted neuron-to-neuron signal can also be acquired with a magnetic sensor.
A magnetoenecephalograph (MEG) consists of several dozens of these sensors distributed around the head.
MEG and EEG are currently the only non-invasive acquisition methods that measure brain activity with a milisecond resolution [1.4.MEG.a][1.4.MEG.b][1.4.MEG.c].
The focus of this project is on cognitive processes that require timespans in the single-digit second range to complete.
Considering these small time frames, a good temporal resolution is integral for yielding a sufficient amount of data from every processing step.
This is the reason why I decided to use a EEG/MEG method, while using the highest available temporal resolution.

\paragraph{Choice of acquisition methods}
Compared to EEG, the MEG-based measuring strategy has a few advantages and drawbacks.

The first difference between MEG and EEG is due to the sensor technology.
While both methods rely on strong amplification of very small input signals, only MEG uses superconducting sensors.
Contemporary superconductors require cooling with liquid helium.
Magnetic fields from neurons are also weaker than environmental magnetic noise by several magnitudes.
To limit measurements to neural activity, the MEG device need to be shielded with large quantities of highly magnetically permeable material (most commonly, an nickel-iron alloy).
These requirements makes MEG much less portable, and equally more expensive, than EEG.
Passive shielding already reduces environmental noise levels by 25-60db [1.4.SNR].
In addition to that, it is possible to dampen noise levels by another 60db with adaptive noise reduction [1.4.SNR].
Good adaptive noise reduction requires large external coils that can counteract outside magnetic fields.
The use of superconducting MEG sensors is necessary due to the extreme weakness of neural magnetic fields.
With the large amplification factors involved in both methods, amplifiers contribute a large amount of noise to the signal.
But since the amplifier noise depends directly on their operating temperature, suspending the amplifiers in liquid helium drastically lowers the noise level [1.4.MEG.a].
Generally, EEG and MEG are considered equally sensitive [1.4.MEG.a][1.4.MEG.c].
However, the addition of magnetic shielding can elevate signal-to-noise ratios in MEG measurements above equivalent acquisitions from EEG [1.4.SNR].

The second difference between MEG and EEG is the drastically different distortion from surrounding tissue.
For an EEG to be able to measure a potential difference on the skin surface, an electrical current needs to pass the tissues surrounding the cortical surface [1.4.tissues.b].
Some of these tissue layers, like blood vessels or cerebreal spinal fluid (CSF), are 5 times more conductive than gray matter; so they smooth and diffuse electric fields [1.4.tissues.a][1.4.tissues.b].
Other tissue layers, especially the compact bone, conduct electricity 78 times worse than gray matter; so they distort and attenuate every passing signal [1.4.tissues.a].
After these tissues have been passed, the original signal has substantially decreased in intensity, and changed drastically in shape and location.
In contrast to electrical fields, the same tissues are highly permeable to magnetic fields.
The magnetic permeability of water, which most human tissue is based on, differs from the magnetic permeability of vacuum only by 0.0008\%.
As a general rule, only metal-based materials are substantially less permeable than water.
Since the human head doesn't contain metal in considerable quantities, it practically allows magnetic fields to pass without distortion. [1.4.tissues.a]

This circumstance does not imply, unfortunately, that every neural transmission arrives at the magnetic sensors with equal strength.
There are three main reasons for that.

First, the main source of electrical and magnetical activity are the pyramidal cells in the cortical tissue on the brain surface.
The activity of a single neuron, besides being highly unlikely in vivo, doesn't create a field with sufficient strength to elict a response in contemporary EEG or MEG sensors.
Only the combined and synchronous activity of larger clusters of neurons can cross the detection threshold.

Second, the human cortex is folded into gyri and sulci.
When two neural groups at opposite cortical walls produce identical activity, the two created electric and magnetic fields are directly opposed to each other.
Opposing fields cancel each other out, so the original signal will be systematically underestimated by surrounding sensors.

Third, their basic physical properties imply that electric and magnetic fields are orthagonal to each other.
Signals that travel along fibers orthogonally to the head surface create the strongest magnetic activation in surrounding sensors, but the weakest voltage in surface electrodes.
Fibers that lie parallel to the head surface, in contrast, create strong electric but weak magnetic activation.
The implication on measurements of neural activity is that EEG is most sensitive for gyri and sulci, and MEG is most sensitive for the radial walls inbetween.
To counteract this particular measurement bias, EEG and MEG data would have to be acquired simultaneously.

Although the simultaneous acqusisition of EEG and MEG data provides theoretical benefits to the signal quality, I ultimately decided against this strategy.
MEG acquisition consists of three preparation steps: Getting written consent, applying the HPI coils and ocular electrodes, and digitizing the head.
This preparation typically requires 25 minutes for children and 15 minutes for experienced adults.
Including EEG acquisition would have added several lengthy steps to this procedure: Fitting the gel electrode cap, ensuring a good connection for each electrode, and plugging in each of the 63 cables individually.
These additional steps would have extended the preparation time to at least 60 minutes.
Since patience is not a strong trait in children\footnote{Implementing details with the goal to prevent boredom was a common theme in this study, as explained in more detail in chapter 3.}, I wanted to minimize any idle waiting times between their arrival and the experiment.
Therefore, I only acquired MEG data during this study.

\subsection{Preprocessing}

Once the signals were acquired by the MEG, there were two necessary decisions for preprocessing.

\paragraph{DC bias removal}
The first decision concerns the removal of DC bias that is often caused by slow sensor drift.
Traditionally, for the exploration of effects in ERP and ERF, a subtractive baseline correction is applied to every trial before averaging.
For this purpose, the average is computed from roughly 100 to 500 (typically 200) miliseconds of activity before the conditional cue.
This initial interval is assumed to originate from brain activity unrelated to the post-cue task.
The computed average value is then subtracted from activity data in the corresponding trial.
This procedure assures that different DC components from long-term trends (for either technical or cognitive reasons) don't disturb the trigger-dependent effect.
However, there is a fundamental issue with the baseline correction.
Because the stimuli are spoken sentences in this study, there is no silent time window around the critical words.
Therefore, the pre-cue interval reflects electric fields from unrelated brain activity.
By computing the average from unrelated activity, I effectively introduces a random DC error into the correction procedure.
This issue makes a subtractive baseline correction a tradeoff between the original DC error and a random DC error.
A process to remove DC components without this tradeoff is to use a highpass filter [1.4.highpass].
Since my longest expected evoked field, the ELAN, has a base frequency of 5Hz, I choose a much lower value of 0.4Hz for the high-pass.

\paragraph{Artifact removal}
The second decision concerns the removal of various measuring artifacts.
There are four major types of artifacts during the acquisition of electric or magnetic fields.

The first type of artifact is caused from cardiac activity.
Cardiac muscles create an almost continuous, very regular field with low frequency and medium strength.

The second type of artifact is caused by ocular movements.
Since the eyeballs are electrically charged, all eye movements are associated with a continuously changing field of low frequency and medium strength.

The third type of artifact is caused by muscle movements.
Muscle activity creates relatively long and strong distortions in a wide frequency band.

The final type of artifact is caused by oversaturation in MEG sensors.
Oversaturation happens randomly when no high pass is in use, and reduces the sensitivity of the affected channel to zero.
This condition is remedied with an automatic reset, which in turn produces a single very short and very large jump in amplitude.

The first two artifacts can be eliminated with the help of three additional acquisition channels.
With electrodes attached to the chest and to the eye sockets, electric fields from ocular and cardiac activity are measured directly.
MEG data is then deconstructed into independent data components with an independent component analysis.
The artifact channels are used to identify artifact components in the measured MEG data.
If the extracted data component is similar enough to one of the measured artifact channels, it is removed. 
The remaining components are then assembled to a data composition, ideally containing no cardiac or ocular artifacts.

The last two artifacts can be removed with a simple threshold detection.
Their high amplitude make it possible to set a manual amplitude threshold, and reject segments that exceed this threshold in any channel.
I determined the threshold manually after visual artifact inspection, and decided to reject entire trials if this threshold is exceeded.

\subsection{Timewindow estimation}
The acquired signals need to be explored for the impact of the conditional effect.
This effect is usually spatially and temporally limited.
For establishing time intervals (TOI) and spatial regions of interest (ROI), there are two possible approaches.

First, existing literature can be consulted for activity effects from syntax contrasts in similar experiments.

Second, a bootstrapping approach can be used.
For this approach, the measured activity is compared between conditions.
The TOI and ROI that involve considerable contrast between conditions can then be selected for the comparison of mean activity.
The drawback to this approach is both spurious contrast and activity from different cognitive processes are considered as condition effect.
The statistical testing will therefore systematically overestimate the condition effect.
This issue is known as "'double dipping"' [1.4.Kriegeskorte].
However, for exploratory analysis, bootstrapping is a valuable tool that can uncover previously unknown TOI and ROI.

I decided to use both approaches with different purposes:
First, comparisons within previously discovered ROI and TOI provide results that can be compared well to the findings of earlier studies.
Second, a bootstrapped comparison allows for the exploration of spatial and temporal properties of the syntactic effect.

\subsection{Source localization}

\paragraph{Motivation}
Magnetic fields, when induced by the brain, arrive at MEG sensors only as mixture of many cortical sources.
Demixing these signals is a fundamentally flawed process.
The problem of discerning these signal sources is equivalent to finding the location and intensity of all the flames in a hot air balloon, while only looking at the outside of the hull.
There are infinitely many possible configurations of light sources that can generate the same brightness pattern on the outer skin.
The same issue is valid for localizing magnetic signal sources in the human head.
This task involves creating a bidirectional map between the curved plane of MEG sensors and the threedimensional human head.
Because of the different dimensionality, the task of creating this map is an underdefined problem.
This means that there are infinitely many possible locations and intensities for magnetic fields that can generate the exact same signal pattern in the MEG sensors.
This multitude of possible solutions needs to be constrained to make the results meaningful.
One popular set of constraints is the use of a source model.

\paragraph{Choice of source model}
A source model assumes that there is a limited number of discrete current sources distributed throughout the brain.
Usually, these current sources are assumed to be generated by neuronal tissue.
There are three popular types of source modelling.

The first type of source modelling uses spatial filtering.
A popular spatial filtering strategy is the single-core beamformer method (Barnes and Hillebrand, 2003; Gross and Ioannides, 1999; Gross et al., 2001; Hillebrand and Barnes, 2003; Robinson and Vrba, 1999; Sekihara et al., 2001; Van Veen et al., 1997).
Its main weakness is the assumption that data from different sources is completely uncorrelated.
This assumption is especially detrimental to the analysis of cortical signals, since neuronal-level synchronizity is one of the fundamental principles behind attention and learning (Kandel et al., 2000).

In the focal source model, neural current flow is represented by a limited set of point-shaped current dipoles.
There are are three subcategories to this model: an unconstrained variant (the moving dipole model), dipoles with a fixed position (the rotating dipole model) and dipoles with a fixed position and rotation (the fixed dipole model).
Popular applications of this approach include "'multiple signal classification"' (MUSIC) [1.4.music] and "'multi-start spatio-temporal multiple-dipole modeling"' [1.4.simplex]
Dipole models have had limited success with representing neuronal responses for two main reasons.
First, reducing extended neuroanatomical structures to a point current source introduces a systematic model error.
Second, the number and location of dipoles has a strong influence on the localized results, yet is hard to estimate in advance (Huang et al., 1998).

The third type of source model is the distributed source model.
For these approaches, a dense grid of dipoles is derived from a cortical layer.
The goal is to place dipoles in homogenous density at every location that is able to produce currents.
Typically, the continuous cortex surface is extracted from anatomical data and populated with several thousands of (roughly) equally spaced dipoles.
For determining localized activity, moments are computed for all dipoles.
The dipole moments are then used to simulate activity in the MEG sensors.
Simulated activity then is optimized so that the error to the reference sensor activity is minimal.
Because every sensor activity pattern can be created by infinitely many source configurations, the localization process is facilitated with two processes.
First, dipole activity is spatially regularized with a predefined factor.
Second, the best pattern of localized sources is selected by minimizing the norm over all dipoles.
The most popular norm today is the L2-norm [1.4.L2], and implementations are widely available.
Alternatively, the L1-norm [1.4.L1a, 1.4.L1b] can result in a more focally reconstructed activity.
This process is usually computed separately for every temporal sample.
Popular implementations include dSPM (Dale et al., 2000), MNE (Hammalaien, 2005) and sLORETA (Pascual-Marqui, 2002)).
I opted for this approach because the spatial filtering approaches aren't recommended for localizing cortical activity, and the quality of results from the dipole fit models depend too strongly on the inital parameters.

L2-norm-based solutions have two major drawbacks.
First, the solution has a relatively low spatial resolution.
This issue leads to spatially distributed activity clusters even if the real sources are very focal.
If the sources are in close proximity, unintended mixing of reconstructed source activity can occur as well.
Second, generic L2-normal solutions contain a mandatory systematical spatial bias.
The sLORETA algorithm, by contrast, has been designed to create solutions with zero bias.
Since this algorithm has minimal drawbacks out of all readiliy available software, it became the method of choice for my source localization purposes.

\subsection{Information transfer}
After localization, activity in my selected cortical regions is ready for the analysis of transfered information.
According to the Wiener principle [1.4.information], information processing consists of three separate tasks: information storage, information modification and information transfer.
In comparison to true causality, which involves all three tasks, the analysis of transfered information is feasible on discrete electrophysiological data. 
There are two fundamentally different approaches to this task.


% "'' information processing can be broken down into the three components of
%  information storage, information transfer, and information modification [1-4]. These components can be
%  easily identified in theoretical, or technical, information processing systems, such as ordinary comput-
%  ers, based on the specialized machinery for and the spatial separation of these component functions. In
%  these examples, a separation of the components of information processing via a specialized mathematical
%  formalism seems almost superfluous. However, in biological systems in general, and in the brain in par-
%  ticular, we deal with a form of distributed information processing based on a large number of interacting
%  agents (neurons), and each agent at each moment in time subserves any of the three component functions
%  to a varying degree. In neural systems it is indeed crucial to understand where and when information
%  storage, transfer and modification take place, to constrain possible algorithms run by the system. While
%  there is still a struggle to properly define a measure for information modification [5,6] and its proper mea-
%  sure [7-10], well established measures for (local active) information storage [11], information transfer [12],
%  and its localization in time [13] exist.
%  Especially the measure for information transfer, transfer entropy (TE), has seen a dramatic surge
%  of interest in neuroscience [14-31], physiology [32-34], and other fields [5, 13, 26, 35, 36]. Nevertheless,
%  conceptual and practical problems still exist. On the conceptual side, information transfer has been for a
%  while confused with causal interactions, and only some recent studies [37-39] made clear that there can be
%  no one-to-one mapping between causal interactions and information transfer, because causal interactions
%  will subserve all three components of information processing (transfer, storage, modification). However,
%  it is information transfer, rather than causal interactions, we might be interested in when trying to
%  understand a computational process in the brain [38]."''

% modelling vs. model-free, linear vs. nonlinear
% Transfer Entropy is suited best for highly complex, nonlinear timescale data
% Potential pitfall: Volume conduction (can also come from bad localization)
% TrenTOOL can correct volume conduction
