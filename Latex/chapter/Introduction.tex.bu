
\chapter{Introduction}

\section{Neurobiological background}
\subsection{Exploration strategies of white matter tracts}
The human brain can be regarded as an information-processing system, processing and storing environmental stimuli in order to create meaningful actions. 
In a very simplified information-processing perspective, it can be reduced to two functional components: computational units in the grey matter and signal relays in the white matter.
Contrary to other common information-theoretical systems, there is no fundamental distinction between hardware and software in the brain.
Both structural elements (e.g., dendritic spines) and computational units (e.g., astroglial cells) are part of an inseparable information process.
The fact that anatomical features can't be clearly mapped to a functional role violates the naive reductionist approach, and puts cognitive science more in line with other fields that explore dynamical systems, such as physics, theoretical computer science or meteorology.
Although the exploration of anatomic features (e.g., the classification of cortical regions based on myeloarchitecture) led to several ground-breaking discoveries in psychology, these findings tend to serve as a base for more specialized studies, rather than creating a framework for understanding cognitive processes.

Most anatomical features offered a direct link between their structure and functional roles, providing a solid basis for functional research.
In the context of the human brain, there was one major exception: cerebral white matter, comprised of myelinated axons and glial cells.
The role of cerebral nerve fibers eluded scientific inquiry for centuries, mainly due to their microscopic structure and the unknown function of their end connection points.
The idea that they played a role in signal transmission (in the loosest sense) can be traced back as early as 1710 \cite{1.1.history}.
In the writings of Pourfoir de Petit, nerve fiber bundles were theorized to transmit animal spirits from the cortex to the medulla oblongata.
The complete motor transmission chain as known today, including the path from from the medulla to the extremities via the spinal cord, was discovered only 40 years later by Emanuel Swedenborg \cite{1.1.history}.
However, during the next 250 years, the exploration of the role of cerebral fiber tracts was stifled by the lack of a non-invasive, large-scale measurement method.

These circumstances abruptly changed in 1972 \cite{1.1.history}, when the discovery of the autoradiographic method allowed tracing the precise interneural connections with radioactive markers.
Radioactive tracing was very widely used to fill existing knowledge gaps in cognitive science, but one fundamental flaw has prevented its application in humans: not only is the tracer very invasive, but the subject needs to be killed and dissected in the course of the experiment.
This drawback has prevented the study of the role of human fiber bundles, and not all knowledge gaps could be filled with equivalent results from lab animals.

One of the most important set of fiber bundles that eluded autoradiographic study was the human language network.
Only with the advent of main-stream diffusion imaging \cite{1.1.firstDiffusion} in the mid-1980s, examinations of human fiber bundles in vivo started to be possible.
The analysis of the previously elusive language-related fiber bundles is still ongoing, while new methods in both acquisition and data processing proceed to advance.
But before I give an overview over the language network, some fundamentals need to be explained first.

\begin{figure}[h]
\begin{center}
\vspace{7mm}
\includegraphics[width=0.75\textwidth]{pics/1_1_myelin_sheath}
\caption{\label{1.1.neuron.illustrated} Illustration of an axonal fiber, wrapped in myelin layers. Adapted from Mosby's Medical Dictionary, 8th edition. \textcopyright 2009, Elsevier.}
\end{center}
\end{figure}

\subsection{The role of myelination}\label{1.1.myelin}
When regarding the brain from an information-theoretical perspective, it is important to consider that it is a self-optimizing structure with two important optimization constraints: the mechanism of myelination and limited space.

The importance of myelination becomes clear when regarding the mechanism of long-distance cerebral signal transmission.
Long-distance nerve fibers typically transmit signals in the form of electric fields.
This transmission process relies on the interplay of ion channels and electric currents.
Since the signal propagation is based on an activation cascade of ion channels, the transmitted signal is strictly binary in nature, is limited in bandwidth by a refractory period of $\approx 1ms$ and can only travel in one direction at a time.
The typical propagation speed of a nerve fiber is only $0.1\frac{m}{s}$, but can be increased by several magnitudes when enveloped by myelin layers.
Myelin layers, produced by myelinating sheath cells, reduce the amount of electrical current lost to the extracellular (usually liquid) environment.
This effect causes not only a higher propagation speed, but also an equally increased maximum signal bandwidth, since the refractory period stays constant.

Myelination causes not only positive aspects, but also creates a major drawback: it increases the volume of the nerve fiber drastically.
For an practical illustration, A-$\gamma$-fibers, responsible for relaying hot and cold sensations, have a diameter of \SI{3}{\um} and propagate signals at $15\frac{m}{s}$.
In comparison, A-$\beta$-fibers, responsible for sensing touch on the skin, have a diameter of \SI{10}{\um} and propagate signals with a speed of $50\frac{m}{s}$.
The cross section of this fiber class is 11 times bigger, although its effective bandwidth is only three times higher.
The volume requirements of myelination follow a law of diminishing returns: a single fiber with a high propagation speed requires more space than two fibers at half the speed, although the combined bandwidth is equivalent.

Since the brain is situated in an inflexible skull cavity, overall volume for the whole network (and, in effect, for every nerve fiber) is fundamentally volume-constrained.
Myelination is flexible and adaptive, so bandwidth can be increased by adding additional myelin layers to fibers in high demand.
Due to the volume constraints, this is only possible when neighboring fibers are reduced in diameter by the same volume.

This combination of constraints has apparently led to the prominent ``small-world``' topology featured in every level of the central nervous system \cite{1.1.smallWorld}.
This type of network is characterized by a large number of very local connections; in fact, almost the majority (47.8\%) of all nodes are only connected to their nearest neighbors.
The small-world topology is a natural result of the trade-off that is created by minimizing the network distance between nodes while using the smallest amount of connections.
It is also responsible for one of the most convenient characteristics in cognitive science, the specialized brain region.
In a strongly local network, neural clusters in close proximity can share their information most effectively in closest network proximity.
And since neural clusters can choose to connect to other clusters most relevant to their task, this network proximity also becomes a functional proximity.
In a small-world-topology, this functional proximity becomes a spatial proximity.
Therefore, the optimal solution for information processing at any network level is to group neuronal clusters with similar tasks into a common spatial region.
As a result, the brain is highly structured into distinct functional regions, and regions in close proximity usually exchange signals with each other.

The combination of volume constraints and myelination properties creates another trade-off in one of the most fundamental tasks of the brain: learning.
Learning, on a neural level, always requires some form of change.
This change can occur within the neuron, for example by tweaking the input weights of established connections.
In other cases, new signal sources are necessary to accomplish the recently learned task.
Finding new signal sources requires movement, either by growing new fiber strands or by repositioning or remyelinating existing fibers.
In extreme cases, even the neuron body can move, at up to 10mm per hour.
Again, due to volume constraints, movement requires space, which is usually acquired by demyelinating existing fibers.
In order to preserve the effective bandwidth even in slower fibers, neural clusters at the end points have the option of compressing the signal.
An example for a compression strategy is to employ two clusters of redundant information at both ends of the fiber, and only transmit the reference to the cluster that contains this information.
Signal compression inevitably increases the local network complexity at the fiber end points, but can still create effective volume savings.
Especially in long-distance connections, fiber diameter has a very large impact on overall volume.
Long-distance connections make up only 2\% of all connections but occupy two thirds of white matter volume \cite{1.1.fiberstats}.

The volume constraints put a constant metaphoric pressure on both length and diameter of long-distance connections.
A highly myelinated bundle of fibers that maintains its length and diameter beyond the first years of development implies that it plays an essential role in the network and that its signals require high bandwidth.

\subsection{The language network}
The language fiber network, as previously mentioned, has been explored only during the recent two centuries.
There are four fiber tracts that seem to be responsible for relaying language-related signals.
All of those four fiber tracts connect regions of the frontal cortex with regions of the temporal cortex.
They can be grouped into two sets, a dorsal and a ventral set \cite{1.1.Gierhan}.

The dorsal set consists of the arcuate fascile (AF) and the superior longitudinal fascile (SLF).
The SLF consists of three individual sections, which connect different pairs of anatomical features.
SLF II connects the angular gyrus with the frontal cortex, SLF III connects the frontal cortex to the supramarginal gyrus and SLF-tp connects the angular gyrus with the temporal cortex.
The AF connects pars opercularis and the posterior superior temporal gyrus (pSTG).

The ventral set consists of the inferior fronto-occipital fascicle (IFOF, also known as ECFS) and the unicate fasicle (UF).
The IFOF connects the frontal cortex with the posterior temporal cortex, as well as to occipital and parietal regions.
The UF connects the anterior interior region of the frontal cortex and the anterior temporal cortex.

The regions at the end points of these fiber tracts are responsible for forwarding and receiving language-related signals.
In order to examine their role in language processing, it is necessary to define these regions with the least amount of functional overlap as possible.
For this purpose, it is helpful to include a definition based on functional roles, rather than purely by their anatomical properties.
This perspective \cite{1.1.pathways} yields much more narrowly defined functional regions, which are still directly connected by the four major pathways.
Because these connections are defined by their functional roles, I'll refer to them as pathways rather than fiber tracts.
There are, as before, four pathways in two groups: two ventral pathways and two dorsal pathways.

\clearpage

\begin{figure}[h]
\begin{center}
\vspace{7mm}
\includegraphics[width=0.49\textwidth]{pics/1_1_tracts_ventral}
\includegraphics[width=0.49\textwidth]{pics/1_1_tracts_dorsal}
\caption{\label{1.1.tracts}Illustration of the most probable course of language-related fiber tracts, adapted from the meta-study by \protect\cite{1.1.Gierhan}. Left: ventral fiber tracts, right: dorsal fiber tracts. Numbers indicate Brodmann areas. Abbreviations: AG - angular gyrus, dPMC - dorsal premotor cortex, FOP - frontal operculum, Fpole - frontal pole, IFOF - inferior fronto-occipital fascicle, ILF - inferior longitudinal fascicle, MdLF - middle longitudinal fascicle, N. N. - nomen nescio, Occ - occipital cortex, Orb - orbitofrontal cortex, pSTG - posterior superior temporal gyrus, MTG - middle temporal gyrus, PTL - posterior temporal lobe, SLF II/III/-tp - second/third/temporoparietal component of superior longitudinal fascicle, SMG - supramarginal gyrus, Tpole - temporal pole, UF - uncinate fascicle, vPMC - ventral premotor cortex.}
\end{center}
\end{figure}

The functional boundaries between the ventral pathways are not entirely clear, since their responsibilities can overlap in certain tasks.
However, there are strong trends for the functional role of each pathway.

Ventral pathway 1 connects the temporal pole with the frontal operculum and the orbifrontal cortex via the UF.
Two of these regions, the frontal operculum and the anterior temporal cortex, show activity when taxed with sentence comprehension.
This activity persists even in the absence of semantic content.
From this discovery, the current consensus was established that ventral pathway I is responsible for the parsing of simple syntax structures.

Ventral pathway 2 connects a series of wide-spread regions via the IFOF.
Starting at the occipital cortex, it connects to the pSTG/MTG region and terminates in three frontal areas: the frontal pole, frontal operculum and BA45.
Most areas connected to this pathway, including BA45, MTG, aSTG and pSTG, are known for their involvement in semantic language processing.
Based on the similarities, ventral pathway 1 was hypothesized to be responsible for semantic processing and comprehension.
This hypothesis was confirmed in a series of studies \cite{1.1.tracts.lesions, 1.1.tracts.interference.a, 1.1.tracts.interference.b}, which showed that semantic deficits appeared exclusively when physically interfering with the IFOF.

Dorsal pathway 1 connects Wernicke's area and the premotor cortex via the SLF.
Wernicke's area is a central structure for language, and is involved during both execution and comprehension of most language-related tasks.
The premotor cortex, in contrast, is mainly responsible for movement planning and recognition.
When these two regions use the dorsal pathway 1, they probably perform two tasks: sensor-to-movement mapping \cite{1.1.D1a} and speech repetition \cite{1.1.D1b}.
These tasks aren't entirely separate, since speech repetition depends on sensor-to-movement mapping for basic comprehension.

Dorsal pathway 2 connects Broca's area and Wernicke's area via the AF.
Broca's area and Wernicke's area typically collaborate in tasks that involve complex syntactical structures \cite{1.1.D1a}.
Dorsal pathway 2 is hypothesized to transport top-down predictions from Broca's area for facilitating input processing.
This hypothesis is not yet confirmed with studies of causal signal flow, since existing literature is either based on tractography in healthy subjects or on lesioned subjects with damage in both white and gray matter \cite{1.1.causality}.

% ``Neuroimaging studies have tried to characterise the neural substrate for representing human action. Many of these studies have followed a different tradition in psychophysics and developmental psychology of investigating the perception of ``biological motion`` - that is, the characteristic articulated motion of chordate animal bodies (e.g. Vaina, Solomon, Chowdhury, Sinha, \& Belliveau, 2001; Grossman \& Blake, 2002; Beauchamp, Lee, Haxby, \& Martin, 2002; Pelphrey, Mitchell, McKeown, Goldstein, Allison, \& McCarthy, 2003) or body and face parts (e.g. Hoffman \& Haxby, 2000; Hooker et al. 2003; Kilts et al. 2003; Pelphrey et al., 2003). Biological motion can also be perceived from the relative motion of just a few dots (``point-light walkers``, Johansson, 1973); if the dots are spatially or temporally rearranged, the percept is destroyed. These neuroimaging studies suggest that one brain region, the posterior superior temporal sulcus, is particularly involved in the representation of biological motion.
% Two sets of recent neuroimaging data suggest that the role of the posterior superior temporal sulcus (pSTS) may extend beyond a response to biological motion, to more abstract representations of intentional action. First, Castelli, Happe, Frith, \& Frith, (2000) and Schultz, Grelotti, Klin, Kleinman, Van der Gaag, Marois, \& Skudlarski, (2003) reported that a region of the pSTS showed a significantly higher response to animations of moving geometric shapes that depicted complex social interactions than to animations depicting inanimate motion. Second, using movies of human actors engaged in structured goal-directed actions (e.g. cleaning the kitchen), Zacks, Braver, Sheridan, Donaldson, Snyder, Ollinger, Buckner, \& Raichle, (2001) found that activity in the pSTS was enhanced when the agent switched from one action to another, suggesting that this region encodes the goal-structure of actions. Both of these results are consistent with a role for a region of pSTS cortex in representing intentional action, and not just biological motion.`` (doi:10.1016/j.neuropsychologia.2004.04.015)


% ``hierarchy construction might be supported by left inferior frontal (particularly BA44/45) and posterior superior temporal regions: (Ben-Shachar et al., 2003, 2004; Caplan et al., 2002; Just et al., 1996; Röder et al., 2002; Stromswold et al., 1996)``

% ``the enhanced inferior frontal (BA 44/45) activation observed for these sentence types is engendered by highly specialized aspects of syntactic processing, namely by syntactic transformations (Ben-Shachar et al., 2003, 2004; Grodzinsky, 2000).``


\begin{figure}[h]
\begin{center}
\vspace{7mm}
\includegraphics[width=0.75\textwidth]{pics/1_1_tracts_function}
\caption{\label{1.1.neuron.illustrated} Most probable course of language-related fiber tracts and their most likely functions, adapted from \protect\cite{1.1.Gierhan}. Numbers indicate Brodmann areas. AF - arcuate fascicle, AG - angular gyrus, dPMC - dorsal premotor cortex, FOP - frontal operculum, Fpole - frontal pole, ILF - inferior longitudinal fascicle, IFOF - inferior fronto-occipital fascicle, MdLF - middle longitudinal fascicle, MTG - middle temporal gyrus, N. N. - nomen nescio, Occ - occipital cortex, Orb - orbitofrontal cortex, pSTG - posterior superior temporal gyrus, PTL - posterior temporal lobe, SLF II/III/-tp - second/third/temporoparietal component of superior longitudinal fascicle, SMG - supramarginal gyrus, Tpole - temporal pole, UF - uncinate fascicle, vPMC - ventral premotor cortex}
\end{center}
\end{figure}

My goal is to extend the narrow knowledge about information flow on dorsal pathway 2 with this study.\section{Experimental paradigm}

My goal was to investigate the question if dorsal pathway 2 is carrying top-down or bottom-up signals while parsing complex syntax.
To reach this goal, there were four questions that needed to be answered.


The first question was which subjects to use.

Existing studies have been predominantly performed on two groups of subjects: adults and infants.

For adults, current evidence indicates that complex syntactic processing involves a combination of the left pSTG/STS and the pars opercularis of the left inferior frontal gyrus in Broca's area.

In infants, complex syntax processing seems to rely more on the ventral pathway.
This different processing approach explains why children are prone to be biased by semantic cues and why their BA45 shows more involvement while parsing complex syntax.
Diffusion imaging studies have explained this systematic bias by showing that the arcuate fascicle, which is used by dorsal pathway 2, isn't fully developed until early childhood.

In order to expand the existing knowledge about functional development, I required an age group that was already able to solve complex grammar, but whose dorsal pathway 2 was not entirely developed yet.
10-year old children are the oldest group which still performs worse than adults.
I selected subjects who were as mature as possible in order to maximize their patience (useful for motion-free anatomical MRI scans) and their endurance for long behavioral sessions (useful for maximizing the amount of trials for my experiment).

Additionally, I selected a similarly sized adult group for analyzing signal interactions in the adult brain.
Ideally, the differences in processing between adults and children can provide new insights into the developmental process of the dorsal pathway 2.


The second question was which task these subjects should be solving.

The literature provides an established and robust way to isolate the dorsal pathway 2 in psychometric experiments.
Object-relative clauses are known to affect regions connected to this pathway more then subject-relative clauses.
Evidence for the locations of affected regions comes predominantly from fMRI studies.
One of those studies \cite{1.3.Constable} used an actor-undergoer paradigm with sentences that differed only in their syntax order:


\begin{center}
\emph{``The biologist - who showed the video - studied the snake.``}
\emph{``The biologist - who the video showed - studied the snake.``}
\end{center}


This paradigm produced a syntax effect in the left BA40 and BA49, as well as in both BA44/BA45 and ventral premotor cortices.

This process was later refined \cite{1.3.Bornkessel} by using a grammatical peculiarity in the German language: changing between subject- and object-relative clauses is possible even with a fixed word order, by changing only a few suffices and pronouns.
In this study, cases E and F contained unambiguous subject- and object-relative clauses with an active verb.


\begin{center}
\emph{``Gestern wurde erzählt, dass der Junge den Lehrern hilft.``}
\emph{``Gestern wurde erzählt, dass dem Junge die Lehrer helfen.``}
\end{center}


The fMRI-based comparison between these cases yielded the following effect locations:
Left hemisphere: IFG (more specifically, the pars opercularis), pSTG, pSTS, inferior frontal junction, ventral premotor cortex.
Both hemispheres: interparietal sulcus.

I adapted this strategy to create German sentences with a very similar structure.
This process is described in detail in chapter \ref{3.1.stimuli.auditory}.


The third question was how to measure and interpret the evoked cerebral activity.
This topic will be covered in detail in section \ref{1.5}.


The fourth and final question was which results to expect from this study.
\section{Hypotheses}

\paragraph{Replicating earlier results}
Since similar paradigms have been used before, my intermediate results can be compared to previous findings.
Behaviorally, children are expected to perform worse than adults both in reaction time and accuracy.

I expect that syntax complexity has a positive impact on regional activity (i.e., higher activity with more complex syntax).

Based on previous fMRI studies, I expect a different locus of effect in both subject groups.
In adults, I expect the pSTG and the left BA44 to be affected most.
In children, the syntax effect will show mostly in the left pSTG and the left BA45.
Compared to adults, the left pSTG will be affected less strongly and less lateralized.

\paragraph{Explorative questions}
The condition effect has rarely been tested in isolation (i.e., without using semantic cues) in 10-year old children.
Based on the studies on slightly younger age groups, I speculate that neither response time nor accuracy should be affected by syntax complexity.

There is no existing study that resolves the temporal and spatial effect of complex syntax in different stages.
Based on existing theory, I expect an initial stage with bottom-up information flow, a second stage with consolidation between bottom-up and top-down processes, and a third stage with the response preparation.
These stages will show the syntax effect in the temporal cortex during the first stage, in both temporal and frontal cortex during the second stage, and show no effect during the third stage.

Based on the notion that adults rely more exclusively on the dorsal pathway 2 than children, I can form three hypotheses about the effect of complex syntax on interaction strength.
First, I expect a lower interaction strength for regions along the dorsal pathway 2, compared to regions along the ventral pathways. The opposite should be true for adults (pathway effect).
Second, I expect trials with complex syntax to cause a higher interaction strength in regions along the dorsal pathway 2, compared to trials with simple syntax (syntax x pathway effect).
Third, in a comparison of interaction strength within dorsal and ventral pathways, I expect complex syntax to cause a larger difference between pathways in adults than in children (syntax x pathway x group effect).

If the syntax effect turns out to be more lateralized in adults than in children, I expect a less lateralized information flow pathway in children as well.\section{Choice of measurement methods}\label{1.5}

In order to test these hypotheses, I needed to decide how to acquire and analyze my subjects' brain activity.
This process is best explained by starting with the basis for cortical signals.

\subsection{Signal creation}
Neuronal activity is represented as a combination of electrical and magnetic fields.
Activating a neuron causes depolarization, which manifests itself in a temporarily positively charged cell.
Most neurons are equipped with a long axonal fibre that propagates this positive charge, the action potential.
The signal propagation is primarily based on a cascade of ion channels.
Ion channels are depolarized by the arriving positive charge, and create a positive charge in response.
This process causes a current that travels along the axon.
As indicated in section \ref{1.1.myelin}, axons can be enveloped with myelin sheaths, which locally bypass the ion channel cascade and cause the signal to be transmitted via direct current flow.
Especially for pyramidal cells, which are typically used for long-distance transmissions, this axon can span several centimeters in length.

Since signals along axons travel by a complex combination of ion channels and electric currents, neuron-to-neuron data transmission exhibits three important properties.
First, a successful signal transmission requires a short refractory period until the next transmission is possible again.
This leads to the phenomenon that transmissions can only travel one way, and overlapping signals on the same fiber are impossible.
Second, axonal transmissions can only be binary.
If the minimum threshold voltage is reached, the signal will be transmitted at maximum speed along the entire fibre.
Below the threshold, no transmission can occur.
Third, maximum transmission speed is relatively low at approximately $100\frac{m}{s}$.
Other commonly used transmission fibers in technical applications, for comparison, achieve speeds of $2\cdot10^8\frac{m}{s}$ (copper wire) or even $3\cdot10^8\frac{m}{s}$ (glass fiber).
Additionally, transmision speed depends on the amount of myelination, with the thinnest fibers transmitting as slowly as 0.1m/s.
This property causes a considerable delay when transmitting signals over macroscopic distances.
Signal delay in technical applications is generally small enough to be disregarded or considered an inescapable nuisance.
In the brain however, two interconnected neurons at opposite sides of the head can generate output simultaneously, but their signals will be offset by several miliseconds by the time they arrive.
For any neural network, delayed information is therefore an widely expected issue and must be properly factored into signal processing.

\subsection{Signal Acquisition}
\paragraph{Available acquisition methods}
Each neuron is supplied with electric currents (the primary currents) from other cells via its dendrites.
The accumulation of these currents creates a positive or negative electric charge in the neuron body, generating an action potential in response.
Nerve fibers are usually embedded in the conductive extracellular fluid.
Each primary current that flows through axons or dendrites induces an opposite electric current, the volume current.
Aggregated volume currents from thousands of simultaneous currents in parallel dendrites are strong enough to be measured with an electroencephalograph (EEG).
This method compares the electric potential at one or more arbitrary points around the brain against a reference point.
Typically, the electrodes for measuring the potentials are connected to the surface of the head, or directly to the cortical surface.
Measuring potentials in cortical electrodes is called electrocorticography.

Every electrical current that travels along a nerve fiber also induces a magnetic field.
Especially primary currents in parallel dendrites create magnetic fields that can be measured by a magnetoenecephalograph (MEG).
A MEG measures magnetic fields by inducing a current in sensing coils that are distributed in short distance around the head.
The sensing coils can have different shapes, with different properties.
The induced current in the sensing coils is transmitted to a coupling coil.
Finally, the magnetic field of the current coil is measured by a SQUID (superconducting quantum interference device) sensor.

MEG and EEG are currently the only non-invasive acquisition methods that measure brain activity with a milisecond resolution \cite{1.5.MEG.a}\cite{1.5.MEG.b}\cite{1.5.MEG.c}.
The focus of this project is on cognitive processes that require timespans in the single-digit second range to complete.
Considering these small time frames, a good temporal resolution is integral for yielding a sufficient amount of data from each processing step.
This is the reason why I decided to use a EEG or MEG method, while using the highest available temporal resolution.

\paragraph{Choice of acquisition methods}
Compared to EEG, the MEG-based measuring strategy has both advantages and drawbacks.


The first difference between MEG and EEG is the sensor technology.
While both methods rely on strong amplification of very small input signals, only MEG uses superconducting sensors.
Contemporary superconductors require cooling with liquid helium.
Magnetic fields from neurons are also weaker than environmental magnetic noise by several magnitudes.
To limit measurements to neural activity, the MEG device needs to be shielded with large quantities of highly magnetically permeable material (most commonly, \si{\micro}-metal - a nickel-iron alloy).
These requirements makes MEG much less portable, and more expensive, than EEG.
Passive shielding already reduces environmental noise levels by 25-60db \cite{1.5.SNR}.
The use of superconducting MEG sensors is necessary due to the extremely low amplitude of neural magnetic fields.
Generally, EEG and MEG are considered equally sensitive \cite{1.5.MEG.a}\cite{1.5.MEG.c}.
A more nuanced view shows that MEG is more sensitive for flat, tangential sources, while EEG favors flat, radial and deep sources \cite{1.5.sensitivity}.
With magnetic shielding, signal-to-noise ratios in MEG measurements can be better than in equivalent acquisitions from EEG \cite{1.5.SNR}.
Finally, constructing a EEG forward model requires the digitization of the exact electrode positions and a coregistration with a MRI-derived head surface.
Since the MEG sensors are fixed in location and orientation, constructing the MEG forward model is more straightforward.


The second difference between MEG and EEG is the drastically different amount of distortion from surrounding tissue.
For an EEG to be able to measure a potential difference on the skin surface, an electrical current needs to pass the tissues surrounding the cortical surface \cite{1.5.tissues.b}.
Some of these tissue layers, like blood vessels or cerebreal spinal fluid (CSF), are 5 times more conductive than gray matter; so they smooth and diffuse electric fields \cite{1.5.tissues.a}\cite{1.5.tissues.b}.
Other tissue layers, especially the compact bone, conduct electricity 78 times worse than gray matter; so they distort and attenuate every passing signal \cite{1.5.tissues.a}.
After these tissues have been passed, the original signal has decreased in intensity by approximately four orders of magnitude, and changed drastically in shape and location.
In contrast to electrical fields, the same tissues are highly permeable to magnetic fields.
The magnetic permeability of water, which most human tissue is based on, differs from the magnetic permeability of air only by 0.0008\%\footnote{All non-metallic materials have a relative magnetic permeability very close to 1 (Air: 1.00000037, Vacuum: exactly 1 (by definition), Water: 0.999992). For the purposes of magnetic imaging, only materials with markedly bigger or smaller values than 1 (e.g., iron with a factor of 5000) affect imaging accuracy to a noticable degree.}\cite{1.5.magneticProperties}.
As a general rule, only metal-based materials are substantially more permeable than water.
Since the human head doesn't contain metal in considerable quantities, it practically allows magnetic fields to pass without distortion \cite{1.5.tissues.a}.

Due to the availability of both methods and the obvious technical advantages of MEG, I chose MEG for the data acquisition for this study.


These favorable properties of MEG however don't imply that every neural transmission yields an equivalent signal at the magnetic sensors.
There are three major reasons for that.

First, the magnetic field of a single neuron (or one of its extremities) is far too weak to be discriminated by the magnetic sensors.
Only the simultaneous and parallel activity of larger clusters of neurons can cross the detection threshold.

Second, the human cortex is folded into gyri and sulci.
When two neural groups at opposite cortical walls produce identical activity, the induced magnetic fields are directly opposed to each other.
Opposing fields eliminate each other before reaching the sensors, so the original signal will be systematically underestimated by surrounding sensors.

Third, their basic physical properties imply that electric and magnetic fields are orthagonal to each other.
Currents along fibers that run radial to the head surface create only a negligible activation in MEG sensors, but the highest electric potential in EEG electrodes.
Fibers that lie parallel to the head surface, in contrast, create strong electric but weak magnetic activation.
To counteract this particular measurement bias, EEG and MEG data would have to be acquired simultaneously.


Although the simultaneous acqusisition of EEG and MEG data provides theoretical benefits to the signal quality, I ultimately decided against this strategy due to two reasons.

First, MEG acquisition consists of three preparation steps: Getting written consent, applying the HPI coils and ocular electrodes, and digitizing the head.
This preparation typically requires 25 minutes for children and 15 minutes for experienced adults.
Including EEG acquisition would have added two lengthy steps to this procedure: Fitting and digitizing the gel electrodes, and additional fine-tuning for an overall low impedance.
These additional steps would have extended the preparation time to 60 minutes.
Patience is not a strong trait in 10-year-old children, and additional preparation meant additional idle waiting time between their arrival and the experiment.
In order to prevent motivational deficits due to boredom and low engagement\footnote{Implementing details with the goal to sustain a high level of motivation and engagement was a common theme in this study, and will be explained in more detail in chapter 3.}, I attempted to keep waiting times as short as possible.

Second, and more importantly, there is no freely available software for the combined analysis of MEG and EEG data streams.
Building two separate processing workflows would have been necessary, which involves the especially difficult tasks of solving two different inverse models, and the combination of two separate sets of reconstructed source activity.
Although promising in principle, the doubled complexity of this procedure would have multiplied the likelihood for issues with parameter optimization and signal compatibility.
Ultimately, the forseeable procedural drawbacks outweighed the incremental increase in data quality.

For these reasons, I only acquired MEG data during this study.

\subsection{Data preprocessing}

Once the signals were acquired by the MEG, there were two necessary decisions for preprocessing.

\paragraph{DC bias removal}
The first decision concerns the removal of DC bias that is inherent to each MEG sensor.
For the exploration of effects in event-related potentials (ERP) or event-related fields (ERF), a baseline correction is applied to every trial before averaging.
For this purpose, the average activity is computed from roughly 100 to 500 (typically 200) miliseconds before the conditional cue.
This time interval is assumed to contain brain activity unrelated to the post-cue task.
The computed average value, an estimation of the momentary DC offset, is then subtracted from activity data in each corresponding trial.
This procedure assures that different DC components from long-term trends (for either technical or cognitive reasons) don't disturb the trigger-dependent effect.

However, there is a fundamental issue with the baseline correction.
The location of the conditional cue in the middle of a sentence (see \ref{3.1.stimuli.auditory}) ensures that there is no silent time interval before or after the trigger.
Therefore, both pre-cue and post-cue intervals result in trigger-locked activity.
By computing the average from pre-cue activity, I would effectively introduce a systematic DC error into the correction procedure.
A process to remove DC components without this error is to use a highpass filter \cite{1.5.highpass}.
I employed a highpass filter of 0.4Hz to preserve the shape of the slowest of my expected evoked components, the P600.

\paragraph{Artifact removal}
The second decision concerns the removal of various measuring artifacts.
There are four major types of artifacts during the acquisition of electric or magnetic fields.

The first type of artifact is caused from cardiac activity.
Cardiac muscles create a very regular electric field pattern, the QRS-complex \cite{1.5.ecg.a}.
Its power spectrum shows two distinct peaks; one between 0.04Hz and 0.12Hz, and one between 0.2Hz and 0.3Hz \cite{1.5.ecg.b}.

The second type of artifact is caused by ocular movements.
Since the eyeballs are electrically charged [?], all eye movements (but especially blinks) introduce an electric field peak with a duration of 30-80ms. This electric field also creates magnetic field artifacts in the same shape and an amplitude of typically 2-3fT.

The third type of artifact is caused by muscle movements.
Muscle activity creates ? [duration] and ? [amplitude] distortions in a wide frequency band.

The final type of artifact is caused by oversaturation in MEG sensors.
Oversaturation occurs randomly when no high pass is in use, and reduces the sensitivity of the affected channel to zero.
This condition is remedied with an automatic reset, which in turn produces a single very short and very large jump in amplitude.

The first two artifacts can be eliminated with the help of three additional acquisition channels.
With electrodes attached to the chest and to the eye sockets, electric fields from ocular and cardiac activity are measured directly.
MEG data is then deconstructed into independent data components with an independent component analysis.
The artifact channels are used to identify artifact components in the measured MEG data.
If the extracted data component is similar enough to one of the measured artifact channels, it is removed. 
The remaining components are then assembled to a data composition, ideally containing no cardiac or ocular artifacts.

The last two artifacts can be removed with a simple threshold detection.
Their high amplitude make it possible to set a manual amplitude threshold, and reject segments that exceed this threshold in any channel.
I determined the threshold manually after visual artifact inspection, and decided to reject entire trials if this threshold is exceeded.

\subsection{Timewindow estimation}
The acquired signals need to be explored for the impact of the experimental conditions.
This effect is usually spatially and temporally limited.
For establishing the temporal and regional extent, there are two possible approaches.

First, existing literature can be consulted for activity effects from syntax contrasts in similar experiments.

Second, a bootstrapping approach can be used.
For this approach, the measured activity is compared between conditions.
The time intervals (TOI) and regions of interest (ROI) that involve considerable contrast between conditions can then be selected for the comparison of mean activity.
The drawback to this approach is both spurious contrast and activity from different cognitive processes are considered as condition effect.
The statistical testing will therefore systematically overestimate the condition effect.
This issue is known as ``double dipping`` \cite{1.5.Kriegeskorte}.

Selecting data purely based on contrast causes spurious differences in activity to be falsely classified as condition effect.
This bias can be prevented with a cluster-level permutation comparison (or ``cluster analysis``).
Cluster analysis relies on the assumption that spurious signal contrasts vary randomly, while the underlying signal contrast between conditions remains at least partially stable across trials.

I decided to use both approaches with different purposes:
First, previously discovered ROI and TOI from similar experimental designs allow my results to be compared well to the findings of earlier studies.
Second, a bootstrapped comparison allows for the exploration of spatial and temporal properties of the syntactic effect.

\subsection{Source localization}

\paragraph{Motivation}
MEG sensors are inherently impartial when measuring magnetic fields.
Due to their high sensitivity, MEG sensors measure a mixture of several types of sources.
``Outside`` (as in ``not contained inside the skull cavity``) sources, such as the earth's magnetic field, can overpower brain activity by several orders of magnitude.
Even though outside sources can be mostly removed during preprocessing, the magnetic fields from different cortical sources strongly overlap at the sensors.
Demixing these signals is necessary, but a fundamentally flawed process.
To illustrate these flaws, consider the task of finding the location and intensity of all the flames inside a hot air balloon, while only looking at the outside of the hull.
There are infinitely many possible configurations of light sources that can generate the same brightness pattern on the hull.
A very similar issue presents itself when trying to determine the momentary intensity and location of neural field sources from a surface measurement.
This task involves creating a bidirectional map between the (two-dimensional) curved plane of MEG sensors and the (three-dimensional) human head.
Because of the different dimensionality, the task of creating this map is an underdefined problem.
This means that there are infinitely many possible locations and intensities for magnetic fields that can generate the exact same signal pattern in the MEG sensors.
This multitude of possible solutions needs to be constrained to make the results meaningful.
Different sets of constraints result in different types of approaches.
There are two popular types of approaches for reconstructing neural activity, filtering methods and inverse methods.

\paragraph{Source reconstruction methods}
The most popular methods are spatial filtering methods.
This type of method estimates the source activity at individual spatial points, instead of finding a global solution.
Currently popular filtering methods are the single-core beamformer method \cite{1.5.Beamformer-a, 1.5.Beamformer-b} and the multiple signal classification \cite{1.5.music}.
Source reconstruction in these methods is performed determine neural activity at a spatial point by modelling it with a dipole.
This reconstruction is repeated for the whole cortex by sweeping over a multitude of points.
Due to this mechanism, these methods are also called scanning methods.
Their main weakness is the assumption that data from different sources is completely uncorrelated.
This assumption is especially detrimental to the analysis of cortical signals.
Neural synchronicity is one of the fundamental principles behind attention and learning \cite{1.5.synchronicity}.
Synchronous signals appear highly correlated when measured, resulting in skewed beamformer results during periods of high synchronicity \cite{1.5.Beamformer.Problems}. 

The second type of source reconstruction, inverse modeling, represents neural current flow with a limited set of point-shaped current dipoles.
There are are three subcategories to this model: an unconstrained type (the moving dipole model), dipoles with a fixed position (the rotating dipole model) and dipoles with a fixed position and rotation (the fixed dipole model).
A popular application of this approach would be, for example, \emph{multi-start spatio-temporal multiple-dipole modeling} \cite{1.5.simplex}.
Dipole models have had limited success with representing neuronal responses for two main reasons.
First, reducing extended neuroanatomical structures to a point current source introduces a systematic model error.
Second, the number and location of dipoles has a strong influence on the localized results, yet is hard to estimate in advance (Huang et al., 1998).

For these approaches, a dense grid of dipoles is derived from a cortical layer.
The goal is to place dipoles in homogenous density at every location that is able to produce currents.
Typically, the continuous cortex surface is extracted from anatomical data and populated with several thousands of (roughly) equally spaced dipoles.
For determining localized activity, moments are computed for all dipoles.
The dipole moments are then used to simulate activity in the MEG sensors.
Simulated activity then is optimized so that the error to the reference sensor activity is minimal.
Because every sensor activity pattern can be created by infinitely many source configurations, the localization process is facilitated with two processes.
First, dipole activity is spatially regularized with a predefined factor.
Second, the best pattern of localized sources is selected by minimizing the norm over all dipoles.
The most popular norm today is the L2-norm \cite{1.5.L2}, and implementations are widely available.
Alternatively, the L1-norm \cite{1.5.L1} can result in a more focally reconstructed activity.
This process is usually computed separately for every temporal sample.
Popular implementations include dSPM \cite{1.5.dSPM}, MNE \cite{1.5.MNE} and sLORETA \cite{1.5.sLORETA}).
I opted for this approach because the spatial filtering approaches aren't recommended for localizing cortical activity, and the quality of results from the dipole fit models depend too strongly on the inital parameters.

L2-norm-based solutions have two major drawbacks.
First, the solution has a relatively low spatial resolution.
This issue leads to spatially distributed activity clusters even if the real sources are very focal.
If the sources are in close proximity, unintended mixing of reconstructed source activity can occur as well.
Second, generic L2-normal solutions contain a mandatory systematical spatial bias.
The sLORETA algorithm, by contrast, has been designed to create solutions with zero bias.
Since this algorithm has minimal drawbacks out of all readiliy available software, it became the method of choice for my source localization purposes.

\subsection{Information transfer}
Cognitive processes are based on the interaction of dynamically and statically coupled neural networks.
Statical coupling is established with nerve fiber connections, mainly representing the upper limit of possible functional connections to other areas.
The other type of coupling are dynamic connections, which establish the exchange of information by synchronizing neuron clusters to a common frequency.
Especially the latter coupling strategy plays a big role in understanding cognitive processes, since dynamic connections are - due to bandwidth constraints - only upheld as long as meaningul information needs to be transfered.
By measuring the time intervals and spatial extends of dynamic connections, it is possible to construct a spatiotemporal graph of interactions between all cortical regions involved in a cognitive task.
My task was to determine two maps of dynamic connections, one for each experimental condition, and compare the differences.

\paragraph{Two types of information interaction}
According to the Wiener principle \cite{1.5.information}, information processing in general consists of three separate tasks: information storage, information modification and information transfer.
This definition is important for determining the task that can be measured with practical methods.
In the literature, a common question is whether a signal transfer was causal or not.
However, reconstructing true causal relationships between signal processors involves measuring all three tasks.
Both information storage and information modification take place on a cellular level.
Since these processes leave few traces in the form of electric or magnetic fields, our only way to resolve their details is by analyzing single cells with microscopic probes.
In contrast to these tasks, information transfer can reconstructed reasonably well from discrete, brain-wide electrophysiological measurements.
Although the practical restrictions limit us to the analysis of information transfer, this measure, rather than true causality, may actually be the more interesting factor for understanding a computational process in the brain \cite{1.5.causality}.

Historically, information transfer has been explored by calculating functional connectivity \cite{1.5.connectivity}.
Functional connectivity is defined as the temporal correlation between activity of different cortical regions.
While this approach is suitable for a wide spectrum of acquisition methods (EEG, MEG, fMRI, PET), it fails to uncover the directionality of any interaction.
For this goal, a relatively new approach is necessary, effective connectivity.
Next to functional and structural (anatomical) connectivity, this is a fundamentally different type of approach.
It explores the influence that one cortical region has over another cortical region, and is uniquely able to determine directional properties of signal interactions.

\paragraph{Choice of analysis approach}
There are two fundamentally different approaches to the analysis of effective connectivity.

The first type of analysis involves fitting each acquired time series to a well-defined model.
These types of models approaches commonly involve on biophysical properties, e.g. varying neuronal firing rates or varying strengths of dendritic connections.
Once the model parameters are established, the implied interactions between the models can be explored.
The commonly used model-based approaches are statistic equation modelling \cite{1.5.SEM} and dynamic causal modelling \cite{1.5.DCM}.
Due to their large parameter space and high computational demands, these approaches are better suited for small networks.
I ultimately decided against a model-based approach because of the non-trivial task of parameter optimization.

The second type of analysis works without an underlying model or other metadata.

\paragraph{Granger-Causality derivatives}
This data-driven approach was incepted with Granger causality \cite{1.5.Granger}, which is built on Wiener's assumption that causes precede their effects in time.
If the future time series of a signal can be better predicted by the past time series of a second signal than by its own past, then the second signal has had a causal effect on the first signal.
There are different implementations within this family of approaches with minor conceptual differences.
Granger causality, for example, can accomodate a maximum of two time series.
An effort to generalize this method to multiple data sources and to frequency space led to the development of the directed transfer function.

Importantly, all following methods are robust against a common effect in realistic electrophysical data: volume conduction \cite{1.5.PDC}.
Since the brain is enveloped in cerebral spinal fluid, a highly conductive material, any cortical activity can spread practically immediately across wide distances.
Due to its small amplitude and almost random content, this effect is negligible for traditional trigger-locked approaches, in which a signal stationarity can be assumed.
However, during the analysis of effective connectivity, volume conduction leads to an instantaneous signal transfer, competing with the much more meaningful signal transfer by axonal fibers.
Volume conduction especially overestimates the amount of signal transfer in EEG data, in which the electrical activity from neuronal clusters are mixed with electric fields in the liquid layer around the cortex.

Because the directed transfer function fails to distinguish between direct and indirect interactions in a multivariate network, the partial directed coherence (PDC) method was developed.
PDC, which represents data as a multivariate autoregressive model, is able to select only direct interactions.
PDC shows a few minor issues, most famously its susceptibility to different levels of amplitude.
All of these issues are resolved with the generalized PDC (gPDC) \cite{1.5.gPDC}.

All derivative approaches of Granger causality can only capture linear interactions between data streams.
This limitation leads to a systematic underestimation of effective connectivity when non-linear interactions are involved in creating the measured activity.

\paragraph{Information-theoretic approaches}
Next to mutual information and joint entropy, transfer entropy (TE) is by far the most advanced information-theoretic approach \cite{1.5.TEcomparison}.
TE is a non-linear, model-free approach, is resistant to indirect causal effects and volume conduction, and incorporates both dynamical and directional information \cite{3.4.TE.a}, \cite{3.4.TE.b}.
It is, just like Granger causality, based on Wiener's assumption of temporal delays between cause and effect.
The core principle is the comparison within and between two streams of entropy.
If the future entropy of one data stream can be explained better by adding entropy from another data stream, an entropy transfer must have occurred.
This entropy transfer is quantified and secured against false positive detection with a statistical comparison against surrogate data.
This surrogate data needs to be constructed so that the causal relationship between data streams is destroyed.
TE also requires relatively large amounts of data that exhibits at least some stationarity.
Both requirements are solved by supplying trigger-locked single trials with generous time limits and a high sampling rate.
The most extensive implementation of TE is the software Trentool \cite{3.4.Trentool}.
Trentool can compute group-level statistics and effects of experimental conditions, making it the ideal software for the goal of this study.